
ğŸ’¡ **ë°ì´í„° ìƒì„±ì´ë‚˜ ì½”ë”©**ì€ **Linux**ì—ì„œ
ë§µë¦¬ë“€ìŠ¤ ì½”ë“œì™€ ì…ë ¥ ë°ì´í„°ë¡œ **ë§µë¦¬ë“€ìŠ¤ ì•Œê³ ë¦¬ì¦˜**ì€ **HDFS**ì—ì„œ


## Hadoop ë””ë ‰í† ë¦¬

- wordcount_test/ :  `ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ë°ì´í„° ë””ë ‰í† ë¦¬`
- wordcount_test_out/ :  `ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë””ë ‰í† ë¦¬`

## ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì»´íŒŒì¼

- **ant**
    - Java ê°œë°œ í™˜ê²½ì—ì„œì˜ í‘œì¤€ ë¹Œë“œ ë„êµ¬
    - src ë””ë ‰í† ë¦¬ì— ìˆëŠ” ê²ƒì„ ë‹¤ ëª¨ì•„ì„œ ì»´íŒŒì¼ í•œ í›„ì— jarì„ ìƒì„±í•œë‹¤.
    - Project ë””ë ‰í† ë¦¬ì— ìˆëŠ” build.xml íŒŒì¼ì— ì •ì˜í•œ ëŒ€ë¡œ ìˆ˜í–‰

## ìƒˆë¡œìš´ ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì»´íŒŒì¼

- ìƒˆë¡œìš´ ì†ŒìŠ¤ ì½”ë“œ íŒŒì¼ì„ Project/src/ ì— ë„£ëŠ”ë‹¤.
- Driver.java íŒŒì¼ì— ì•„ë˜ ì½”ë“œë¥¼ ë„£ëŠ”ë‹¤.
    
    `pgd.addClass(â€program nameâ€, class name, â€œdescriptionâ€);`
    
- Project ë””ë ‰í† ë¦¬ì—ì„œ ant ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•œë‹¤.

## Linux í•˜ë‘¡ í”„ë¡œì íŠ¸ êµ¬ì¡°

```bash
Project
â”œâ”€â”€ **build.xml** # ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì»´íŒŒì¼ì„ ìœ„í•œ íŒŒì¼
â”œâ”€â”€ **data** # ê³¼ì œë¥¼ ìœ„í•œ ë°ì´í„°
â”‚Â Â  â”œâ”€â”€ data-knn-R.txt
â”‚Â Â  â”œâ”€â”€ data-knn-S.txt
â”‚Â Â  â”œâ”€â”€ equijoin-R-data.txt
â”‚Â Â  â”œâ”€â”€ equijoin-S-data.txt
â”‚Â Â  â”œâ”€â”€ kmeans-data.txt
â”‚Â Â  â”œâ”€â”€ knnjoin-data.txt
â”‚Â Â  â”œâ”€â”€ labelratio-data.txt
â”‚Â Â  â”œâ”€â”€ matadd-data-2x2.txt
â”‚Â Â  â”œâ”€â”€ matmulti-data-2x2.txt
â”‚Â Â  â”œâ”€â”€ setjoin-data.txt
â”‚Â Â  â”œâ”€â”€ topksearch-data.txt
â”‚Â Â  â”œâ”€â”€ vecsimjoin-data.txt
â”‚Â Â  â””â”€â”€ wordcount-data.txt
â”œâ”€â”€ **datagen** # ê³¼ì œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì½”ë“œ
â”‚Â Â  â”œâ”€â”€ KMeansGenerator.java
â”‚Â Â  â”œâ”€â”€ MatrixGenerator.java
â”‚Â Â  â”œâ”€â”€ SetGenerator.java
â”‚Â Â  â”œâ”€â”€ TwoWayJoinGenerator.java
â”‚Â Â  â””â”€â”€ VecSimJoinGenerator.java
â”œâ”€â”€ set_hadoop_env.sh
â”œâ”€â”€ **src** # ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ
â”‚Â Â  â”œâ”€â”€ Driver.java # ë§µë¦¬ë“€ìŠ¤ ì½”ë“œ ì»´íŒŒì¼ì„ ìœ„í•œ íŒŒì¼
â”‚Â Â  â”œâ”€â”€ Wordcount1char.java
â”‚Â Â  â””â”€â”€ Wordcount.java
â”œâ”€â”€ **ssafy.jar** # ë¹Œë“œëœ jar íŒŒì¼
â””â”€â”€ **template** # ê³¼ì œë¥¼ ìœ„í•œ template
    â”œâ”€â”€ AllPairJoin.template.java
    â”œâ”€â”€ AllPairPartitionSelf.template.java
    â”œâ”€â”€ AllPairPartition.template.java
    â”œâ”€â”€ CommonItemCount.template.java
    â”œâ”€â”€ KMeans.template.java
    â”œâ”€â”€ KNNJoin.template.java
    â”œâ”€â”€ LabelRatio.template.java
    â”œâ”€â”€ MatrixAdd.template.java
    â”œâ”€â”€ MatrixMulti1.template.java
    â”œâ”€â”€ MatrixMulti2.template.java
    â”œâ”€â”€ MatrixMulti3.template.java
    â”œâ”€â”€ MatrixMulti.template.java
    â”œâ”€â”€ RPJoinSort.template.java
    â”œâ”€â”€ RPJoin.template.java
    â”œâ”€â”€ SetSimJoin.template.java
    â”œâ”€â”€ TopKSearch.template.java
    â”œâ”€â”€ VecSimJoin.template.java
    â””â”€â”€ VecSimTopKJoin.template.java
```

## Wordcount

```java
package ssafy;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class Wordcount {

	/* 
	**Map í•¨ìˆ˜**
	Object, Text : input key-value pair type (always same (to get a line of input file))
	Text, IntWritable : output key-value pair type
	**<Map í•¨ìˆ˜ ì…ë ¥ KEY íƒ€ì…, Map í•¨ìˆ˜ ì…ë ¥ VALUE íƒ€ì…, Map í•¨ìˆ˜ ì¶œë ¥ KEY íƒ€ì…, Map í•¨ìˆ˜ ì¶œë ¥ VALUE íƒ€ì…>**
	*/
	public static class TokenizerMapper
			extends Mapper<Object,Text,Text,IntWritable> {
		
		// **ë³€ìˆ˜ ì„ ì–¸**
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();

		// map function (Context -> fixed parameter)
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// value.toString() : get a line
			StringTokenizer itr = new StringTokenizer(value.toString());
			while ( itr.hasMoreTokens() ) {
				word.set(itr.nextToken());

				// **KEY-VALUE ìŒì„ emit**
				context.write(word,one);
			}
		}
	}

	/*
	**Reduce í•¨ìˆ˜**
	Text, IntWritable : input key type and the value type of input value list
	Text, IntWritable : output key-value pair type
	*/
	public static class IntSumReducer
			extends Reducer<Text,IntWritable,Text,IntWritable> {

		// variables
		private IntWritable result = new IntWritable();

		// **key : a disticnt word**
		// **values :  IntWritable íƒ€ì…ì˜ ë¦¬ìŠ¤íŠ¸**
		public void reduce(Text key, Iterable<IntWritable> values, Context context) 
				throws IOException, InterruptedException {

			int sum = 0;
			for ( IntWritable val : values ) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key,result);
		}
	}

	/* 
	**Main í•¨ìˆ˜** 
	*/
	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration(); // job ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì„¤ì • ì´ˆê¸°í™”
		String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();

		if ( otherArgs.length != 2 ) {
			System.err.println("Usage: <in> <out>");
			System.exit(2);
		}

		Job job = new Job(conf,"word count"); // job ì‘ì„±, ë”°ì˜´í‘œ ì•ˆì€ ì„¤ëª… ì“°ë©´ ë¨
		job.setJarByClass(Wordcount.class); // jobì„ ìˆ˜í–‰í•  class ì„ ì–¸, íŒŒì¼ëª….class, ëŒ€ì†Œë¬¸ì ì£¼ì˜

		job.setMapperClass(TokenizerMapper.class); // Map class ì„ ì–¸, ìœ„ì—ì„œ ì‘ì„±í•œ class ëª…
		job.setReducerClass(IntSumReducer.class); // Reduce class ì„ ì–¸
		// job.setCombinerClass(IntSumReducer.class); Combiner class ì„ ì–¸

		// Reduceì˜ outputì˜ keyì™€ value íƒ€ì… ì§€ì •
		job.setOutputKeyClass(Text.class); // Output key type ì„ ì–¸
		job.setOutputValueClass(IntWritable.class); // Output value type ì„ ì–¸
		// Mapê³¼ Reduceì˜ output ê°™ì„ ë•ŒëŠ” Map outputì— ëŒ€í•œ ì§€ì • ìƒëµ ê°€ëŠ¥
		// job.setMapOutputKeyClass(Test.class);
		// job.setMapOutputValueClass(IntWritable.class);

		job.setNumReduceTasks(2); // ë™ì‹œì— ìˆ˜í–‰ë˜ëŠ” reducer ê°œìˆ˜

		FileInputFormat.addInputPath(job,new Path(otherArgs[0])); // ì…ë ¥ ë°ì´í„°ê°€ ìˆëŠ” path
		FileOutputFormat.setOutputPath(job,new Path(otherArgs[1])); // ê²°ê³¼ë¥¼ ì¶œë ¥í•  path
		System.exit(job.waitForCompletion(true) ? 0 : 1 ); // ì‹¤í–‰
	}
}
```

## ì½”ë“œ ì‹¤í–‰í•˜ê¸°

- í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ HDFSì— ë„£ê¸°

```bash
# í•˜ë‘¡ì˜ hdfsì— wordcount_test ë””ë ‰í† ë¦¬ ìƒì„±
hdfs dfs -mkdir wordcount_test

# Linuxì˜ data ë””ë ‰í† ë¦¬ì— ìˆëŠ” wordcount_data.txt íŒŒì¼ì„ hdfsì˜ wordcount_test ë””ë ‰í† ë¦¬ì— ë³´ëƒ„
hdfs dfs -put data/wordcount_data.txt wordcount_test

# ë°˜ë“œì‹œ ë§µë¦¬ë“€ìŠ¤ í”„ë¡œê·¸ë¨ì´ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•œ í›„ í”„ë¡œê·¸ë¨ ì‹¤í–‰í•˜ê¸°
hdfs dfs -rm -r wordcount_test_out

# Wordcount MapReduce ì•Œê³ ë¦¬ì¦˜ ì½”ë“œ ì‹¤í–‰
# Driver.javaì— í‘œì‹œí•œ ëŒ€ë¡œ wordcountë¥¼ ì¨ì„œ Wordcount ë§µ ë¦¬ë“€ìŠ¤ ì½”ë“œ ìˆ˜í–‰
# Wordcount_test ë””ë ‰í† ë¦¬ì— ë“¤ì–´ìˆëŠ” ëª¨ë“  íŒŒì¼ì„ ë§µ í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
hadoop jar ssafy.jar wordcount wordcount_test wordcount_test_out

# ê²°ê³¼ í™•ì¸ (reducer ê°œìˆ˜ë¥¼ 2ê°œ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì¶œë ¥ íŒŒì¼ 2ê°œê°€ ìƒì„±ëœë‹¤)
hdfs dfs -cat wordcount_test_out/part-r-00000 | more
hdfs dfs -cat wordcount_test_out/part-r-00001 | more
```

## Main í•¨ìˆ˜ì—ì„œ Mapperë‚˜ Reducerì— ê°’ì„ Broadcastí•˜ëŠ” ë°©ë²•

```java
public static void main(String[] args) throws Exception {
	
	...
	job.setOutputValueClass(IntWritable.class); // Output value type ì„ ì–¸

	// ë„˜ê²¨ì¤„ ì¸ì setting
	Configuration config = job.getConfiguration();
	config.set("name","Shim"); // Nameì´ë¼ëŠ” ì‹¬ë³¼ì˜ ê°’ì€ "Shim" ì´ë¼ëŠ” ìŠ¤íŠ¸ë§
	config.setInt("one",1); // oneì´ë¼ëŠ” ì‹¬ë³¼ì˜ ê°’ì€ int íƒ€ì…ì´ê³  ê°’ì€ 1
	config.setFloat("point_five", (float) 0.5); // point_fiveë¼ëŠ” ì‹¬ë³¼ì€ float íƒ€ì…ì´ê³  ê°’ì€ 0.5

	FileInputFormat.addInputPath(job, new Path(otherArgs[0])); // ì…ë ¥ ë°ì´í„°ê°€ ìˆëŠ” path
	FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); // ê²°ê³¼ë¥¼ ì¶œë ¥í•  path
	System.exit(job.waitForCompletion(true) ? 0 : 1); // ì‹¤í–‰

}
```
